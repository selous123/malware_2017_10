from __future__ import division
#coding:utf-8
ROOT_PATH = "/mnt/hgfs/ubuntu14/dataset/malware/malware_2017_9_7/img"
#ROOT_PATH = "/home/lrh/dataset/cifar-unbalance-data/"
import sys
sys.path.append("..")
from config import DefaultConfig

import torch.utils.data as data
import numpy as np
import os
import cv2
conf = DefaultConfig()
#from sklearn.model_selection import train_test_split
def read_data_sets(dirpath):
    #list all file name in the folder dir.
    #images = np.empty(shape=[0,128,128])
    #labels = np.empty(shape=[0])
    
    malware_path = os.path.join(dirpath,"malware")
    normal_path = os.path.join(dirpath,"normal")
    
    paths = os.listdir(malware_path)
    malware_images = np.zeros([len(paths),conf.size,conf.size],dtype=np.float32)
    for ind,path in enumerate(paths):
        malware_images[ind] = cv2.resize(cv2.imread(os.path.join(malware_path,path),cv2.IMREAD_GRAYSCALE),(conf.size,conf.size))/256
        if ind%1000==0:
            print "ind is {}".format(ind)
            
    paths = os.listdir(normal_path)
    normal_images = np.zeros([len(paths),conf.size,conf.size],dtype=np.float32)
    for ind,path in enumerate(paths):
        print os.path.join(normal_path,path)
        normal_images[ind] = cv2.resize(cv2.imread(os.path.join(normal_path,path),cv2.IMREAD_GRAYSCALE),(conf.size,conf.size))/256
        if ind%1000==0:
            print "ind is {}".format(ind)
    
    #concatenate malware and normal images,and reshape the images to [,channel,height,width]
    images = np.reshape(np.concatenate((malware_images,normal_images),axis=0),newshape=[-1,1,conf.size,conf.size])
    labels = np.zeros(shape=(normal_images.shape[0]+malware_images.shape[0],1))
    labels[0:malware_images.shape[0],0] = 1
    return images,labels


def read_single_file(file_path):
    image = cv2.resize(cv2.imread(file_path,cv2.IMREAD_GRAYSCALE),(conf.size,conf.size))/256
    
    image = image[np.newaxis,:]
    label = np.zeros(shape=[1])
    if file_path.split("/")[-2] == "malware":
        label[0] = 1
    return image,label
##read the whole file
class malwareData(data.Dataset):
    """
    init dataset class
    Args  :
        root : root path
        train: is trainging or test
    Return:
        self.root,self.train,self.data,self.labels
        
    """
    def __init__(self,root,train):
        self.root = root
        self.train = train
        #train dataset
        if self.train:
            train_path = os.path.join(self.root,"train")
            self.train_data,self.train_labels = read_data_sets(train_path)
        #test dataset
        else:
            test_path = os.path.join(self.root,"test")
            self.test_data,self.test_labels = read_data_sets(test_path)

    
    def __getitem__(self,index):
        if self.train:
            return self.train_data[index],self.train_labels[index]
        else:
            return self.test_data[index],self.test_labels[index]
    
    def __len__(self):
        if self.train:
            return len(self.train_data)
        else:
            return len(self.test_data)
        

##read single file once
class malwareDataset(data.Dataset):
    def __init__(self,root,train):
        self.root = root
        self.train = train
        
        if self.train:
            self.imgs = [os.path.join(self.root,"train","normal",path) for path in os.listdir(os.path.join(self.root,"train","normal"))]
            self.imgs.extend([os.path.join(self.root,"train","malware",path) for path in os.listdir(os.path.join(self.root,"train","malware"))])
        else:
            self.imgs = [os.path.join(self.root,"test","normal",path) for path in os.listdir(os.path.join(self.root,"test"))]
            self.imgs.extend([os.path.join(self.root,"test","malware",path) for path in os.listdir(os.path.join(self.root,"test","malware"))])
    
    def __getitem__(self,index):
        image,label = read_single_file(self.imgs[index])
        return image,label
    
    def __len__(self):
        return len(self.imgs)
    
if __name__=='__main__':
    train_dataset = malwareDataset(conf.root_path,train=True)
    dataloader = data.DataLoader(train_dataset,batch_size=256,shuffle=True,drop_last = True)
    dataiter = iter(dataloader)
    images,labels = dataiter.next()
# =============================================================================
#     for epoch in range(10):
#         print "epoch:{}".format(epoch)
#         for images,labels in dataiter:
#             print "hello world"
# =============================================================================
    
    print images.shape
    print images[0]
    #print labels
    #print np.sum(labels.numpy()==1)
    
    
#%%








